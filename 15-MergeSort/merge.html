<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Merge Sort</title>
</head>

<body>
    <h1>Overview</h1>

    <p>The next three algorithms we will be looking at will be more complex, but the payoff is that they can be far
        faster. Colt did a demo where he sorted 100000 items with bubble and it took about 20s. Merge sort did the same
        100000 items in less than one second.</p>

    <p>It is okay that I can't write them the first time. They are hard. No one needs to reimplement these on there own.
        They are known quantities to everyone. You look them up when you need them. But they come up on interviews so I
        at least need to be able to reconstruct them and understand them.</p>

    <p>As we will see, the more time efficient the algorithm is, the less simple the code will be. The less easy it will
        be to understand because they are not how humans think or would sort things on their own.</p>

    <h2>MergeSort</h2>

    <p>Combination of two things - merging and sorting</p>

    <p>Exploits the fact that arrays of 0 or 1 element are always sorted. Works by decomposing an array into smaller
        arrays of 0 or 1 elements, then building up a newly sorted array. 'Divide and Conquer'.</p>

    <p>Time complexity of O(n log n). The first part in which we slice the arrays over and over is the O(log n), and the
        second part where we merge is O(n). There are no edge cases where it will perform better like there are with
        Bubble
        or Insertion.</p>

    <p>As it turns out, O(n log n) is the mathematical best time complexity any sorting algorithm can be without
        exploiting a strange quirk of numbers (see Radix sort).</p>














</body>

</html>